{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a09d4fb-60c5-4f45-b844-8c788a50c543",
    "_uuid": "8e892e637f005dd61ec7dcb95865e52f3de2a77f"
   },
   "source": [
    "# Titanic: Machine Learning from Disaster w/ Amazon SageMaker\n",
    "\n",
    "Based on https://github.com/minsuk-heo/kaggle-titanic\n",
    "\n",
    "\n",
    "### Predict survival on the Titanic\n",
    "- Defining the problem statement\n",
    "- Collecting the data\n",
    "- Exploratory data analysis\n",
    "- Feature engineering\n",
    "- Modelling\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: ML process\n",
    "![ml_process](./images/ml_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4af5e83d-7fd8-4a61-bf26-9583cb6d3476",
    "_uuid": "65d04d276a8983f62a49261f6e94a02b281dbcc9"
   },
   "source": [
    "## 1. Defining the problem statement\n",
    "Complete the analysis of what sorts of people were likely to survive.  \n",
    "In particular, we ask you to apply the tools of machine learning to predict which passengers survived the Titanic tragedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/5095eabce4b06cb305058603/5095eabce4b02d37bef4c24c/1352002236895/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f529075-7f9b-40ff-a79a-f3a11a7d8cbe",
    "_uuid": "64ca0f815766e3e8074b0e04f53947930cb061aa"
   },
   "source": [
    "## 2. Collecting the data\n",
    "\n",
    "training data set and testing data set are given by Kaggle\n",
    "you can download from  \n",
    "my github [https://github.com/minsuk-heo/kaggle-titanic/tree/master](https://github.com/minsuk-heo/kaggle-titanic)  \n",
    "or you can download from kaggle directly [kaggle](https://www.kaggle.com/c/titanic/data)  \n",
    "\n",
    "### load train, test dataset using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e58a3f06-4c2a-4b87-90de-f8b09039fd4e",
    "_uuid": "46f0b12d7bf66712642e9a9b807f5ef398426b83"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/sidkwon/kaggle-titanic/master/input/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/sidkwon/kaggle-titanic/master/input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "836a454f-17bc-41a2-be69-cd86c6f3b584",
    "_uuid": "1ed3ad39ead93977b8936d9c96e6f6f806a8f9b3"
   },
   "source": [
    "## 3. Exploratory data analysis\n",
    "Printing first 5 rows of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "749a3d70-394c-4d2c-999a-4d0567e39232",
    "_uuid": "b9fdb3b19d7a8f30cd0bb69ae434e04121ecba93"
   },
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "- Survived: \t0 = No, 1 = Yes  \n",
    "- pclass: \tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd  \t\n",
    "- sibsp:\t# of siblings / spouses aboard the Titanic  \t\n",
    "- parch:\t# of parents / children aboard the Titanic  \t\n",
    "- ticket:\tTicket number\t\n",
    "- cabin:\tCabin number\t\n",
    "- embarked:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ebc1e0e-2b5a-4d92-98e0-defa019d4439",
    "_uuid": "1892fbb34b26d775d1c428fdb7b6254449286b28"
   },
   "source": [
    "**Total rows and columns**\n",
    "\n",
    "We can see that there are 891 rows and 12 columns in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed1e7849-d1b6-490d-b86b-9ca71dfafc7d",
    "_uuid": "5a641beccf0e555dfd7b9a53a17188ea6edef95b"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "418b8a69-f2aa-442d-8f45-fa8887190938",
    "_uuid": "4ee2591110660a4a16b3da7a7530f0945e121b46"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "abc3c4fc-6419-405f-927a-4214d2c73eec",
    "_uuid": "622d4d4b2ba8f77cc537af97fc343d4cd6de26b2"
   },
   "source": [
    "We can see that **Age** value is missing for many rows. \n",
    "\n",
    "Out of 891 rows, the **Age** value is present only in 714 rows.\n",
    "\n",
    "Similarly, **Cabin** values are also missing in many rows. Only 204 out of 891 rows have *Cabin* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0663e2bb-dc27-4187-94b1-ff4ff78b68bc",
    "_uuid": "3bf74de7f2483d622e41608f6017f2945639e4df"
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "176aa52d-fde8-42e6-a3ee-db31f8b0ca49",
    "_uuid": "b48a9feff6004d783960aa1b32fdfde902d87e21"
   },
   "source": [
    "There are 177 rows with missing *Age*, 687 rows with missing *Cabin* and 2 rows with missing *Embarked* information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8553d48-c5e0-4947-bd13-1b38509c850c",
    "_uuid": "1a28e607e9ed63cefe0f35a4e4d72f2f36299323"
   },
   "source": [
    "### import python lib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1d8a6d2-c22d-435c-8c98-973e8f41b138",
    "_uuid": "26411c710f69b29939c815d5f5ab01d9177df7d0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set() # setting seaborn default for plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart for Categorical Features\n",
    "- Pclass\n",
    "- Sex\n",
    "- SibSp ( # of siblings and spouse)\n",
    "- Parch ( # of parents and children)\n",
    "- Embarked\n",
    "- Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart(feature):\n",
    "    survived = train[train['Survived']==1][feature].value_counts()\n",
    "    dead = train[train['Survived']==0][feature].value_counts()\n",
    "    df = pd.DataFrame([survived,dead])\n",
    "    df.index = ['Survived','Dead']\n",
    "    df.plot(kind='bar',stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chart confirms **Women** more likely survivied than **Men**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chart confirms **1st class** more likely survivied than **other classes**  \n",
    "The Chart confirms **3rd class** more likely dead than **other classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chart confirms **a person aboarded with more than 2 siblings or spouse** more likely survived  \n",
    "The Chart confirms **a person aboarded without siblings or spouse** more likely dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Parch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chart confirms **a person aboarded with more than 2 parents or children** more likely survived  \n",
    "The Chart confirms **a person aboarded alone** more likely dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chart confirms **a person aboarded from C** slightly more likely survived  \n",
    "The Chart confirms **a person aboarded from Q** more likely dead  \n",
    "The Chart confirms **a person aboarded from S** more likely dead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "810cd964-24eb-44fb-9e7b-18bbddd4900f",
    "_uuid": "fd86ccdf2d1248b79c68365444e96e46a50f3f5a"
   },
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge of the data  \n",
    "to create features (**feature vectors**) that make machine learning algorithms work.  \n",
    "\n",
    "feature vector is an n-dimensional vector of numerical features that represent some object.  \n",
    "Many algorithms in machine learning require a numerical representation of objects,  \n",
    "since such representations facilitate processing and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 how titanic sank?\n",
    "sank from the bow of the ship where third class rooms located  \n",
    "conclusion, **Pclass** is key feature for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/t/5090b249e4b047ba54dfd258/1351660113175/TItanic-Survival-Infographic.jpg?format=1500w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Name' 컬럼에서 영어 호칭을 추출하기 위해 pandas의 str.extract() 메소드를 사용한다.  \n",
    "- str.extract(): https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html  \n",
    "- RegExr: https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train, test] # combining train and test dataset\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False) # expand=True; return dataframe, expand=False; return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title map\n",
    "Mr : 0  \n",
    "Miss : 1  \n",
    "Mrs: 2  \n",
    "Others: 3  \n",
    "\n",
    "pandas.Series.map: https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n",
    "                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n",
    "                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unnecessary feature from dataset\n",
    "# pandas.DataFrame.drop: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop\n",
    "train.drop('Name', axis=1, inplace=True)\n",
    "test.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sex\n",
    "\n",
    "male: 0\n",
    "female: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping = {\"male\": 0, \"female\": 1}\n",
    "for dataset in train_test_data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(sex_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 some age is missing\n",
    "Let's use Title's median age for missing Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing age with median age for each title (Mr, Mrs, Miss, Others)\n",
    "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "test[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.DataFrame.groupby\n",
    "![](./images/pandas_groupby.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = train.groupby(\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform: 데이터프레임에 그룹 단위로 통계량을 집계해서 칼럼 추가  \n",
    "df.groupby(['group']).col.transform('count')\n",
    "\n",
    "![](./images/pandas_groupby_transform.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(30)\n",
    "train.groupby(\"Title\")[\"Age\"].transform(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**seaborn.FacetGrid**  \n",
    "Multi-plot grid for plotting conditional relationships.  \n",
    "https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "\n",
    "**seaborn.kdeplot**\n",
    "Plot univariate or bivariate distributions using kernel density estimation.  \n",
    "https://seaborn.pydata.org/generated/seaborn.kdeplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue: train dataframe을 \"Survived\" 컬럼 기준으로 나눈다\n",
    "# height: 각 facet의 height(in inches)\n",
    "# aspect: 각 facet의 화면비율, aspect=4는 4:1을 의미\n",
    "facet = sns.FacetGrid(train, hue=\"Survived\", height=4, aspect=4)\n",
    "\n",
    "# facet.map: Facet grid에 kdeplot을 그린다 (Parameter: 사용할 그래프 이름, 시각화를 진행할 dataframe의 컬럼명, 채우기(fill)여부 )\n",
    "facet.map(sns.kdeplot,'Age',shade=True)\n",
    "\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    " \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(30, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(40, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(40, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, train['Age'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Binning\n",
    "Binning/Converting Numerical Age to Categorical Variable  \n",
    "\n",
    "feature vector map:  \n",
    "child: 0  \n",
    "young: 1  \n",
    "adult: 2  \n",
    "mid-age: 3  \n",
    "senior: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandas_selection](./images/pandas_selection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\n",
    "Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\n",
    "Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\n",
    "df = pd.DataFrame([Pclass1, Pclass2, Pclass3])\n",
    "df.index = ['1st class','2nd class', '3rd class']\n",
    "df.plot(kind='bar',stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more than 50% of 1st class are from S embark  \n",
    "more than 50% of 2nd class are from S embark  \n",
    "more than 50% of 3rd class are from S embark\n",
    "\n",
    "**fill out missing embark with S embark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "for dataset in train_test_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing Fare with median fare for each Pclass\n",
    "train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\n",
    "test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare',shade= True)\n",
    "facet.set(xlim=(0, train['Fare'].max()))\n",
    "facet.add_legend()\n",
    " \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare',shade= True)\n",
    "facet.set(xlim=(0, train['Fare'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare',shade= True)\n",
    "facet.set(xlim=(0, train['Fare'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare',shade= True)\n",
    "facet.set(xlim=(0, train['Fare'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 100, 'Fare'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\n",
    "Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\n",
    "Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\n",
    "\n",
    "df = pd.DataFrame([Pclass1, Pclass2, Pclass3])\n",
    "df.index = ['1st class','2nd class', '3rd class']\n",
    "\n",
    "df.plot(kind='bar',stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling을 위해 작은 숫자를 사용\n",
    "cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\n",
    "for dataset in train_test_data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing Fare with median fare for each Pclass\n",
    "train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\n",
    "test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 FamilySize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'FamilySize',shade= True)\n",
    "facet.set(xlim=(0, train['FamilySize'].max()))\n",
    "facet.add_legend()\n",
    "plt.xlim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "for dataset in train_test_data:\n",
    "    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop = ['Ticket', 'SibSp', 'Parch']\n",
    "train = train.drop(features_drop, axis=1)\n",
    "test = test.drop(features_drop, axis=1)\n",
    "train = train.drop(['PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.drop('Survived', axis=1)\n",
    "target = train['Survived']\n",
    "\n",
    "train_data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 Dataframe 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(\"train.pkl\")\n",
    "train_data.to_pickle(\"train_data.pkl\")\n",
    "target.to_pickle(\"target.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Amazon SageMaker - Using SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker XGBoost Algorithm에서 요구하는 Input/Output interface 참조\n",
    "The SageMaker implementation of XGBoost supports CSV and libsvm formats for training and inference:\n",
    "- For Training ContentType, valid inputs are text/libsvm (default) or text/csv.\n",
    "- For Inference ContentType, valid inputs are text/libsvm (default) or text/csv.\n",
    "- For CSV training, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record.\n",
    "- For CSV inference, the algorithm assumes that CSV input does not have the label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker에서 요구하는 input(text/csv)로 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  Age  Fare  Cabin  Embarked  Title  FamilySize\n",
       "0         0       3    0  1.0   0.0    2.0         0      0         0.4\n",
       "1         1       1    1  3.0   2.0    0.8         1      2         0.4\n",
       "2         1       3    1  1.0   0.0    2.0         0      1         0.0\n",
       "3         1       1    1  2.0   2.0    0.8         0      2         0.4\n",
       "4         0       3    0  2.0   0.0    2.0         0      0         0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_pickle('train.pkl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  0.0  3.0  0.0  1.0  0.0  2.0  0.0  0.0  0.4\n",
       "1    1    1    1  3.0  2.0  0.8    1    2  0.4\n",
       "2    1    3    1  1.0  0.0  2.0    0    1  0.0\n",
       "3    1    1    1  2.0  2.0  0.8    0    2  0.4\n",
       "4    0    3    0  2.0  0.0  2.0    0    0  0.0\n",
       "5    0    3    0  2.0  0.0  2.0    2    0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove header record from train dataframe\n",
    "all_data = train.copy()\n",
    "new_header = all_data.iloc[0]  #grab the first row for the header\n",
    "all_data = all_data[1:]        #take the data less the header row\n",
    "all_data.columns = new_header  #set the header row as the df header\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total#: 890\n",
      "Train#: 640\n",
      "Validation#: 161\n",
      "Test#: 89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "non_test_sm, test_sm = train_test_split(all_data, test_size=0.1)\n",
    "train_sm, validation_sm = train_test_split(non_test_sm, test_size=0.2)\n",
    "\n",
    "print('Total#: {}'.format(len(all_data)))\n",
    "print('Train#: {}'.format(len(train_sm)))\n",
    "print('Validation#: {}'.format(len(validation_sm)))\n",
    "print('Test#: {}'.format(len(test_sm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sm.to_csv('./train_sm.csv', index=False)\n",
    "validation_sm.to_csv('./validation_sm.csv', index=False)\n",
    "test_sm.to_csv('./test_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker                           # Amazon SageMaker Python SDK\n",
    "import boto3                               # Amazon Python SDK\n",
    "from sagemaker.session import Session      # Amazon SageMaker Python SDK Session 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data, Validation data를 S3에 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Bucket 생성\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "\n",
    "class sagemaker.session.Session(boto_session=None, sagemaker_client=None, sagemaker_runtime_client=None, sagemaker_featurestore_runtime_client=None, default_bucket=None)\n",
    "\n",
    "- boto_session (boto3.session.Session) – The underlying Boto3 session which AWS service calls are delegated to (default: None). **If not provided, one is created with default AWS configuration chain.**\n",
    "- default_bucket (str) – The default Amazon S3 bucket to be used by this session. This will be created the next time an Amazon S3 bucket is needed (by calling default_bucket()). **If not provided, a default bucket will be created based on the following format: “sagemaker-{region}-{aws-account-id}”. Example: “sagemaker-my-custom-bucket”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket() # Region별 default bucket이 없다면 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (참고) SageMaker Session object 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.boto_session.profile_name\n",
    "# session.boto_session.get_credentials().access_key\n",
    "# session.boto_session.get_credentials().secret_key\n",
    "# dir(session)\n",
    "# session.account_id()\n",
    "# session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'titanic-sm-xgboost/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = 'train_sm.csv'\n",
    "validation_file_name = 'validation_sm.csv'\n",
    "test_file_name = 'test_sm.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 upload 방법1) - Amazon Python SDK (boto3) 사용\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3_client.upload_file(train_file_name, bucket, os.path.join(prefix, 'train', train_file_name))\n",
    "s3_client.upload_file(validation_file_name, bucket, os.path.join(prefix, 'validation', validation_file_name))\n",
    "s3_client.upload_file(test_file_name, bucket, os.path.join(prefix, 'test', test_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 upload 방법2) - Amazon SageMaker Python SDK 사용\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/session.html?highlight=upload_data#sagemaker.session.Session.upload_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/test/test_sm.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.upload_data(train_file_name, bucket=bucket, key_prefix=prefix + '/train')\n",
    "session.upload_data(validation_file_name, bucket=bucket, key_prefix=prefix + '/validation')\n",
    "session.upload_data(test_file_name, bucket=bucket, key_prefix=prefix + '/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 upload 방법3) - AWS CLI 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train_sm.csv to s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/train/train_sm.csv\n",
      "upload: ./validation_sm.csv to s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/validation/validation_sm.csv\n",
      "upload: ./test_sm.csv to s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/test/test_sm.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {train_file_name} s3://{bucket}/{prefix}/train/{train_file_name}\n",
    "!aws s3 cp {validation_file_name} s3://{bucket}/{prefix}/validation/{validation_file_name}\n",
    "!aws s3 cp {test_file_name} s3://{bucket}/{prefix}/test/{test_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Estimator로 학습, 모델 배포\n",
    "**Estimator**: A high level interface for SageMaker training - https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\n",
    "\n",
    "class sagemaker.estimator.Estimator(image_uri, role, instance_count=None, instance_type=None, volume_size=30, volume_kms_key=None, max_run=86400, input_mode='File', output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, hyperparameters=None, tags=None, subnets=None, security_group_ids=None, model_uri=None, model_channel_name='model', metric_definitions=None, encrypt_inter_container_traffic=False, use_spot_instances=False, max_wait=None, checkpoint_s3_uri=None, checkpoint_local_path=None, enable_network_isolation=False, rules=None, debugger_hook_config=None, tensorboard_output_config=None, enable_sagemaker_metrics=None, profiler_config=None, disable_profiler=False, environment=None, max_retry_attempts=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) SageMaker training job 이름 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_job_name_base = 'titanic-sm-xgboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SageMaker built-in algorithm의 Docker Registry Paths를 찾는다\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris # XGBoost container image의 주소 [ECR주소]/[알고리즘명]:[태그] 리턴\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework = 'xgboost',\n",
    "    version = '1',\n",
    "    region = boto3.session.Session().region_name\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) SageMaker 학습 후 Model artifacts가 저장될 S3 경로 선언\n",
    "- 지정하지 않으면 default bucket을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/model'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_model_output_location = 's3://{}/{}/model'.format(bucket, prefix)\n",
    "s3_model_output_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) SageMaker training instance가 S3에 업로드 한 데이터를 다운로드 하거나 학습한 모델을 S3에 업로드할 때 사용하는 IAM Role 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::889750940888:role/sinjoonk-sagemaker-demo-execution-role'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) XGBoost hyperparameter 선언, Estimator 생성\n",
    "https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    " 'max_depth': 5,\n",
    " 'objective': 'binary:logistic',\n",
    " 'eta': 0.1,\n",
    " 'subsample': 0.7,\n",
    " 'num_round': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(image_uri=container,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.xlarge',\n",
    "                                          output_path=s3_model_output_location,\n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          base_job_name=sm_job_name_base,\n",
    "                                          sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 학습을 위해 SageMaker channel(Input 데이터의 정의) 생성\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/train\n",
      "s3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/validation\n"
     ]
    }
   ],
   "source": [
    "s3_training_file_location = 's3://{}/{}/train'.format(bucket, prefix)\n",
    "s3_validation_file_location = 's3://{}/{}/validation'.format(bucket, prefix)\n",
    "print(s3_training_file_location)\n",
    "print(s3_validation_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "   'S3Uri': 's3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/train',\n",
       "   'S3DataDistributionType': 'FullyReplicated'}},\n",
       " 'ContentType': 'csv'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_input_config = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_training_file_location,\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='csv',\n",
    ")\n",
    "training_input_config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "   'S3Uri': 's3://sagemaker-us-east-1-889750940888/titanic-sm-xgboost/data/validation',\n",
       "   'S3DataDistributionType': 'FullyReplicated'}},\n",
       " 'ContentType': 'csv'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_input_config = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_validation_file_location,\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='csv',\n",
    ")\n",
    "validation_input_config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) fit() 메소드로 SageMaker 학습 시작\n",
    "- https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html?highlight=sagemaker.estimator.Estimator%20fit#sagemaker.estimator.Estimator.fit\n",
    "\n",
    "    - (str) the S3 location where training data is saved, or a file:// path in\n",
    "local mode.\n",
    "    - (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput]) If using multiple\n",
    "channels for training data, you can specify a dict mapping channel names to strings or TrainingInput() objects.\n",
    "    - (sagemaker.inputs.TrainingInput) - channel configuration for S3 data sources\n",
    "that can provide additional information as well as the path to the training dataset. See sagemaker.inputs.TrainingInput() for full details.\n",
    "    - (sagemaker.session.FileSystemInput) - channel configuration for\n",
    "a file system data source that can provide additional information as well as the path to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 07:16:05 Starting - Starting the training job...\n",
      "2021-09-27 07:16:29 Starting - Launching requested ML instancesProfilerReport-1632726965: InProgress\n",
      "......\n",
      "2021-09-27 07:17:30 Starting - Preparing the instances for training......\n",
      "2021-09-27 07:18:30 Downloading - Downloading input data...\n",
      "2021-09-27 07:19:03 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-09-27:07:19:04:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-09-27:07:19:04:ERROR] Customer Error: Channelname 'train' is required for training\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/amazon/lib/python3.7/site-packages/sage_xgboost/train.py\", line 41, in main\n",
      "    standalone_train(resource_config, train_config, data_config)\n",
      "  File \"/opt/amazon/lib/python3.7/site-packages/sage_xgboost/train_methods.py\", line 17, in standalone_train\n",
      "    train_job(resource_config, train_config, data_config)\n",
      "  File \"/opt/amazon/lib/python3.7/site-packages/sage_xgboost/train_helper.py\", line 300, in train_job\n",
      "    validate_channel_name(channels)\n",
      "  File \"/opt/amazon/lib/python3.7/site-packages/sage_xgboost/train_helper.py\", line 245, in validate_channel_name\n",
      "    raise CustomerError(\"Channelname 'train' is required for training\")\u001b[0m\n",
      "\u001b[34msg_algorithms_sdk.base.exceptions.CustomerError: Channelname 'train' is required for training\u001b[0m\n",
      "\u001b[34m[2021-09-27:07:19:04:INFO] Path /opt/ml/input/data/train does not exist!\u001b[0m\n",
      "\n",
      "2021-09-27 07:19:31 Uploading - Uploading generated training model\n",
      "2021-09-27 07:19:31 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job titanic-sm-xgboost-2021-09-27-07-16-05-688: Failed. Reason: ClientError: Channelname 'train' is required for training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-565983e4da77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train_n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtraining_input_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalidation_input_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3724\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3282\u001b[0m                 ),\n\u001b[1;32m   3283\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m             )\n\u001b[1;32m   3286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job titanic-sm-xgboost-2021-09-27-07-16-05-688: Failed. Reason: ClientError: Channelname 'train' is required for training"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train_n':training_input_config, 'validation':validation_input_config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1) 모델 Deployment - Estimator로 학습까지 끝냈을 경우\n",
    "- sagemaker.estimator.Estimator object의 deploy() method를 사용한다.\n",
    "- deploy(): Amazon SageMaker의 호스팅 서비스를 사용하여 모델을 호스팅 한다. deploy()는 아래 세 과정을 한 번에 처리해 준다. \n",
    "    - Create model: Model artifacts(in S3), Container\n",
    "    - Create endpoint configuration: ProductionVariants(여기에서 model지정), DataCaptureConfig, AsyncInferenceConfig\n",
    "    - Create endpoint: Endpoint configuration을 이용하여 실제 Endpoint 생성\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html?highlight=deploy#sagemaker.estimator.Estimator.deploy\n",
    "\n",
    "deploy(initial_instance_count, instance_type, serializer=None, deserializer=None, accelerator_type=None, endpoint_name=None, use_compiled_model=False, wait=True, model_name=None, kms_key=None, data_capture_config=None, tags=None, **kwargs)\n",
    "\n",
    "- model_name (str) – Name to use for creating an Amazon SageMaker model. If not specified, the estimator generates a default job name based on the training image name and current timestamp.\n",
    "- endpoint_name (str) – Name to use for creating an Amazon SageMaker endpoint. If not specified, the name of the training job is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    # Serialize input data of various formats (a NumPy array, list, file, or buffer) to a CSV-formatted string.\n",
    "    # We use this because the XGBoost algorithm accepts input files in CSV format\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    # wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2) [Optional] 모델 Deployment - 기존에 학습한 모델만 활용할 경우\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/inference/model.html?highlight=model#sagemaker.model.Model\n",
    "\n",
    "class sagemaker.model.Model(image_uri, model_data=None, role=None, predictor_cls=None, env=None, name=None, vpc_config=None, sagemaker_session=None, enable_network_isolation=False, model_kms_key=None, image_config=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_job_name = estimator.latest_training_job.name\n",
    "#sm_job_name = '[YOUR_TRAINING_JOB_NAME]'\n",
    "sm_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker training job의 S3 model artifact (model data) 리턴\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "response = sm_client.describe_training_job(\n",
    "    TrainingJobName=sm_job_name\n",
    ")\n",
    "\n",
    "model_data = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "xgb_model = sagemaker.model.Model(\n",
    "    image_uri=container,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=session # Not required? \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_from_model = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    # Serialize input data of various formats (a NumPy array, list, file, or buffer) to a CSV-formatted string.\n",
    "    # We use this because the XGBoost algorithm accepts input files in CSV format\n",
    "    serializer=sagemaker.serializers.CSVSerializer()\n",
    "    # wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Real-time 추론\n",
    "https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html?highlight=predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sm = pd.read_csv(test_file_name, names=['Survived','Pclass','Sex','Age','Fare','Cabin','Embarked','Title','FamilySize'])\n",
    "test_sm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_survival(id):\n",
    "    prediction_request=test_sm.iloc[id][1:]\n",
    "    prediction=predictor.predict(prediction_request)\n",
    "    print('Passenger ID: {} - Ground Truth: {} / Predicted: {}'.format(id, test_sm.iloc[id][0], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "passenger_id = random.randrange(0, len(test_sm)+1)\n",
    "predict_survival(passenger_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transform\n",
    "After you train a model, you can use Amazon SageMaker Batch Transform to perform inferences with the model. Batch transform manages all necessary compute resources, including launching instances to deploy endpoints and deleting them afterward. You can read more about SageMaker Batch Transform in the AWS documentation.\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html#sagemaker.transformer.Transformer.transform\n",
    "\n",
    "class sagemaker.transformer.Transformer(model_name, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, max_concurrent_transforms=None, max_payload=None, tags=None, env=None, base_transform_job_name=None, sagemaker_session=None, volume_kms_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform 결과가 저장될 S3경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_output_path = 's3://{}/{}/transform'.format(bucket, prefix)\n",
    "transformer_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform의 input으로 사용할 파일 생성 후 S3에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input_file_name = 'test_wo_target.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv(test_file_name, names=['Survived','Pclass','Sex','Age','Fare','Cabin','Embarked','Title','FamilySize'])\n",
    "test_wo_target = test_raw.iloc[:, 1:] # 첫번째 열('Survived')는 제외\n",
    "test_wo_target.to_csv(transform_input_file_name, header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.upload_data(transform_input_file_name, bucket=bucket, key_prefix=prefix + '/transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_input_data = 's3://{}/{}/transform/{}'.format(bucket, prefix, transform_input_file_name)\n",
    "transformer_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {transformer_input_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법1) Estimator를 만들고 Training까지 끝난 경우\n",
    "\n",
    "transformer(instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, role=None, volume_kms_key=None, vpc_config_override='VPC_CONFIG_DEFAULT', enable_network_isolation=None, model_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    output_path=transformer_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform(data, data_type='S3Prefix', content_type=None, compression_type=None, split_type=None, job_name=None, input_filter=None, output_filter=None, join_source=None, experiment_config=None, model_client_config=None, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    data=transformer_input_data,\n",
    "    data_type='S3Prefix',\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법2) Estimator가 없는 경우\n",
    "class sagemaker.transformer.Transformer(model_name, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, max_concurrent_transforms=None, max_payload=None, tags=None, env=None, base_transform_job_name=None, sagemaker_session=None, volume_kms_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_from_model=sagemaker.transformer.Transformer(\n",
    "    model_name='xgboost-2021-09-14-15-14-33-552',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    output_path=transformer_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_from_model.transform(\n",
    "    data=transformer_input_data,\n",
    "    data_type='S3Prefix',\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
